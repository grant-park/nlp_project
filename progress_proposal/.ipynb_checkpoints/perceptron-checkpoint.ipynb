{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys,random,os,csv, random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn import linear_model\n",
    "from sklearn import neural_network\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_subtract(vec1, vec2):\n",
    "    \"\"\"treat vec1 and vec2 as dict representations of sparse vectors\"\"\"\n",
    "    out = defaultdict(float)\n",
    "    out.update(vec1)\n",
    "    for k in vec2: \n",
    "        if k not in out:\n",
    "            out[k] = 0\n",
    "        out[k] -= vec2[k]\n",
    "    return dict(out)\n",
    "\n",
    "def dict_add(vec1, vec2):\n",
    "    out = defaultdict(float)\n",
    "    out.update(vec1)\n",
    "    for k in vec2:\n",
    "        if k not in out:\n",
    "            out[k] = 0\n",
    "        out[k] += vec2[k]\n",
    "    return dict(out)\n",
    "\n",
    "def dict_dotprod(d1, d2):\n",
    "    \"\"\"Return the dot product (aka inner product) of two vectors, where each is\n",
    "    represented as a dictionary of {index: weight} pairs, where indexes are any\n",
    "    keys, potentially strings.  If a key does not exist in a dictionary, its\n",
    "    value is assumed to be zero.\"\"\"\n",
    "    smaller = d1 if len(d1)<len(d2) else d2  # BUGFIXED 20151012\n",
    "    total = 0\n",
    "    for key in smaller.keys():\n",
    "        total += d1.get(key,0) * d2.get(key,0)\n",
    "    return total\n",
    "\n",
    "def dict_const_mult(vec, const):\n",
    "    out = defaultdict(float)\n",
    "    out.update(vec)\n",
    "    for k in vec: out[k] = out[k] * const\n",
    "    return dict(out)\n",
    "\n",
    "def dict_argmax(dct):\n",
    "    return max(dct.keys(), key=lambda k: dct[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_accuracy_vs_iteration(accuracies, num_iter):\n",
    "    plt.plot(range(num_iter), accuracies)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Num. iterations vs Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_label_stats(label_counts):        \n",
    "    labels = []\n",
    "    counts = []\n",
    "    for k in label_counts.keys():\n",
    "        labels.append(k)\n",
    "        counts.append(label_counts[k])\n",
    "        \n",
    "    print labels\n",
    "    print counts\n",
    "    print \"\"\n",
    "    print \"\"\n",
    "    plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class Perceptron(object):\n",
    "    \n",
    "#     def __init__(self, eta=0.01, n_iter=50):\n",
    "#         self.eta = eta #learning rate eta\n",
    "#         self.n_iter = n_iter #number of epochs (passes)\n",
    "#         self._weights = defaultdict(float)\n",
    "        \n",
    "#     def fit(self, train_tweets, gold_labels, test_tweets=None, test_gold_labels=None):\n",
    "        \n",
    "#         # self._errors = [] # array of misclassifications in every epoch (pass)\n",
    "#         accuracies = []\n",
    "#         train_data = [(t, l) for t, l in zip(train_tweets, gold_labels)]\n",
    "#         for i in range(self.n_iter):\n",
    "#             random.shuffle(train_data)\n",
    "#             for tweet, gold_label in train_data:\n",
    "#                 predict_label = self.predict(tweet)\n",
    "                \n",
    "#                 if predict_label != gold_label:\n",
    "#                     pred_feats = features_per_tweet(tweet, predict_label)\n",
    "#                     gold_feats = features_per_tweet(tweet, gold_label)\n",
    "                    \n",
    "#                     diff_feats = dict_subtract(gold_feats, pred_feats)\n",
    "#                     update = dict_const_mult(diff_feats, self.eta)\n",
    "#                     self._weights.update(dict_add(self._weights, update))\n",
    "                \n",
    "#             print\n",
    "#             print\n",
    "#             print \"Iter. \", i, \" - TRAIN RAW EVAL\"\n",
    "#             accuracy = self.do_evaluation(train_tweets, gold_labels)\n",
    "#             accuracies.append(accuracy)\n",
    "#             if test_tweets and test_gold_labels:\n",
    "#                 print(\"Iter. \", i, \" - TEST  RAW EVAL\")\n",
    "#                 self.do_evaluation(test_tweets, test_gold_labels)\n",
    "                \n",
    "#         plot_accuracy_vs_iteration(accuracies, self.n_iter)\n",
    "#         return self\n",
    "    \n",
    "#     def do_evaluation(self, tweets, labels):\n",
    "#         num_correct, num_total = 0.0,0.0\n",
    "#         for tweet, label in zip(tweets, labels):\n",
    "#             predict_label = self.predict(tweet)\n",
    "#             num_correct += 1 if predict_label == label else 0\n",
    "#             num_total += 1\n",
    "#         print(\"%d/%d = %.4f accuracy\" % (num_correct, num_total, num_correct/float(num_total)))\n",
    "#         return num_correct/num_total\n",
    "        \n",
    "#     def predict(self, tweet):\n",
    "#         # \"one-vs-all\" strategy later\n",
    "#         label_dict = {}\n",
    "#         hillary_features = features_per_tweet(tweet, \"Hillary Clinton\")\n",
    "#         hillary_linear_comb = dict_dotprod(hillary_features, self._weights)\n",
    "#         label_dict[\"Hillary Clinton\"] = hillary_linear_comb\n",
    "        \n",
    "#         atheism_features = features_per_tweet(tweet, \"Atheism\")\n",
    "#         atheism_linear_comb = dict_dotprod(hillary_features, self._weights)\n",
    "#         label_dict[\"Atheism\"] = atheism_linear_comb\n",
    "        \n",
    "#         climate_features = features_per_tweet(tweet, \"Climate Change is a Real Concern\")\n",
    "#         climate_linear_comb = dict_dotprod(hillary_features, self._weights)\n",
    "#         label_dict[\"Climate Change is a Real Concern\"] = climate_linear_comb\n",
    "        \n",
    "#         donald_features = features_per_tweet(tweet, \"Donald Trump\")\n",
    "#         donald_linear_comb = dict_dotprod(hillary_features, self._weights)\n",
    "#         label_dict[\"Donald Trump\"] = donald_linear_comb\n",
    "        \n",
    "#         feminist_features = features_per_tweet(tweet, \"Feminist Movement\")\n",
    "#         feminist_linear_comb = dict_dotprod(hillary_features, self._weights)\n",
    "#         label_dict[\"Feminist Movement\"] = feminist_linear_comb\n",
    "        \n",
    "#         abortion_features = features_per_tweet(tweet, \"Legalization of Abortion\")\n",
    "#         abortion_linear_comb = dict_dotprod(hillary_features, self._weights)\n",
    "#         label_dict[\"Legalization of Abortion\"] = abortion_linear_comb\n",
    "        \n",
    "#         return dict_argmax(label_dict)\n",
    "    \n",
    "#         if hillary_linear_comb > len(tweet):\n",
    "#             return \"Hillary Clinton\"\n",
    "#         elif atheism_linear_comb > len(tweet):\n",
    "#             return \"Atheism\"\n",
    "#         elif climate_linear_comb > len(tweet):\n",
    "#             return \"Climate Change is a Real Concern\"\n",
    "#         elif feminist_linear_comb > len(tweet):\n",
    "#             return \"Feminist Movement\"\n",
    "#         elif abortion_linear_comb > len(tweet):\n",
    "#             return \"Legalization of Abortion\"\n",
    "#         else:\n",
    "#             return \"Donald Trump\"\n",
    "        \n",
    "                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def features_per_tweet(tweet, label):\n",
    "#     features = {}\n",
    "#     tweet_tokens = tweet.lower().split()\n",
    "#     for token in tweet_tokens:\n",
    "#         if token not in features:\n",
    "#             features[token] = 1\n",
    "#         else:\n",
    "#             features[token] += 1\n",
    "            \n",
    "#         if \"token=%s_label=%s\" % (token, label) not in features:\n",
    "#             features[\"token=%s_label=%s\" % (token, label)] = 5\n",
    "#         else:\n",
    "#             features[\"token=%s_label=%s\" % (token, label)] += 1\n",
    "            \n",
    "#     # bi-gram features\n",
    "#     for i in range(len(tweet_tokens)-1):\n",
    "#         if \"prevToken=%s_nextToken=%s\" % (tweet_tokens[i], tweet_tokens[i+1]) not in features:\n",
    "#             features[\"prevToken=%s_nextToken=%s\" % (tweet_tokens[i], tweet_tokens[i+1])] = 5\n",
    "#         else:\n",
    "#             features[\"prevToken=%s_nextToken=%s\" % (tweet_tokens[i], tweet_tokens[i+1])] += 1\n",
    "        \n",
    "#         if \"prevToken=%s_nextToken=%s_label=%s\" % (tweet_tokens[i], tweet_tokens[i+1], label) not in features:\n",
    "#             features[\"prevToken=%s_nextToken=%s_label=%s\" % (tweet_tokens[i], tweet_tokens[i+1], label)] = 5\n",
    "#         else:\n",
    "#             features[\"prevToken=%s_nextToken=%s_label=%s\" % (tweet_tokens[i], tweet_tokens[i+1], label)] += 1\n",
    "           \n",
    "#     # trigram features\n",
    "#     for i in range(len(tweet_tokens)-2):\n",
    "#         if \"token1=%s_token2=%s_token3=%s\" % (tweet_tokens[i], tweet_tokens[i+1], tweet_tokens[i+2]) not in features:\n",
    "#             features[\"token1=%s_token2=%s_token3=%s\" % (tweet_tokens[i], tweet_tokens[i+1], tweet_tokens[i+2])] = 1\n",
    "#         else:\n",
    "#             features[\"token1=%s_token2=%s_token3=%s\" % (tweet_tokens[i], tweet_tokens[i+1], tweet_tokens[i+2])] += 1\n",
    "        \n",
    "#         if \"token1=%s_token2=%s_token3=%s_label=%s\" % (tweet_tokens[i], tweet_tokens[i+1], tweet_tokens[i+2], label) not in features:\n",
    "#             features[\"token1=%s_token2=%s_token3=%s_label=%s\" % (tweet_tokens[i], tweet_tokens[i+1], tweet_tokens[i+2], label)] = 5\n",
    "#         else:\n",
    "#             features[\"token1=%s_token2=%s_token3=%s_label=%s\" % (tweet_tokens[i], tweet_tokens[i+1], tweet_tokens[i+2], label)] += 1\n",
    "#     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def process_file(file_path, start=None, end=None):\n",
    "#     tweets, gold_labels, stances = [], [], []\n",
    "#     label_counts = {\"Hillary Clinton\": 0,\"Atheism\": 0,\"Climate Change is a Real Concern\": 0,\"Feminist Movement\": 0,\"Legalization of Abortion\": 0,\"Donald Trump\": 0}\n",
    "    \n",
    "#     with open(file_path, 'r') as f:\n",
    "#         lines = f.read().splitlines()\n",
    "#         num_tweets = len(lines) - 1 # not counting header\n",
    "    \n",
    "#         start = start if start else 1\n",
    "#         end = end + 1 if end else len(lines)\n",
    "#         for line in lines[start:end]:\n",
    "#             fields = line.split('\\t')\n",
    "#             tweets.append(fields[2])\n",
    "#             gold_labels.append(fields[1])\n",
    "#             stances.append(fields[3])\n",
    "#             label_counts[fields[1]] += 1\n",
    "#             num_tweets += 1\n",
    "            \n",
    "#             ##### CHECK #####\n",
    "# #             print \"Gold label\", fields[1]\n",
    "# #             print \"Tweet\", fields[2]\n",
    "# #             print \"Stance\", fields[3]\n",
    "# #             print\n",
    "\n",
    "#     return tweets, gold_labels, label_counts, stances, num_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PerceptronSKLearn(object):\n",
    "    def __init__(self):\n",
    "        self.targets = ['Atheism', 'Legalization of Abortion', 'Feminist Movement', 'Climate Change is a Real Concern', 'Hillary Clinton']\n",
    "        self.target_counts = defaultdict(int)\n",
    "        self.total_count = 0\n",
    "        self.feature_count = defaultdict(int)\n",
    "        self.X_train = []\n",
    "        self.X_test = []\n",
    "        self.y_train = []\n",
    "        self.y_test = []\n",
    "        \n",
    "#     def vectorize_data(self, train_file, start=None, end=None):\n",
    "#         with open(train_file, 'r') as f:\n",
    "#             lines = f.read().splitlines()\n",
    "#             start = start if start else 1\n",
    "#             end = end+1 if end else len(lines)\n",
    "#             for line in lines[start:end]:\n",
    "#                 fields = line.split('\\t')\n",
    "#                 self.total_count += 1\n",
    "#                 self.target_counts[fields[1]] += 1\n",
    "#                 self.X_train.append(fields[2])\n",
    "#                 self.y_train.append(fields[1])\n",
    "#         self.X_train = np.array(self.X_train)\n",
    "#         self.y_train = np.array(self.y_train)\n",
    "            \n",
    "    def prep_data(self, train_file, start=None, end=None):\n",
    "        data = []\n",
    "        conv_dict = {'Atheism':0, 'Legalization of Abortion':1, 'Feminist Movement':2,\n",
    "                        'Climate Change is a Real Concern':3, 'Hillary Clinton':4}\n",
    "\n",
    "        # read data\n",
    "        with open(train_file, 'r') as f:\n",
    "            lines = f.read().splitlines()\n",
    "            start = start if start else 1\n",
    "            end = end+1 if end else len(lines)\n",
    "            for line in lines[start:end]:\n",
    "                fields = line.split('\\t')\n",
    "                self.total_count += 1\n",
    "                self.target_counts[fields[1]] += 1\n",
    "                tokens = fields[2].lower().split()\n",
    "                curr_tweet_feature_count = defaultdict(int)\n",
    "                \n",
    "                # unigram counts\n",
    "                for token in tokens:\n",
    "                    tt_pair_key = \"token=%s_target=%s\" % (token, fields[1])\n",
    "                    self.feature_count[token] += 1\n",
    "                    self.feature_count[tt_pair_key] += 1\n",
    "                    curr_tweet_feature_count[token] += 1\n",
    "                    curr_tweet_feature_count[tt_pair_key] += 1\n",
    "                    \n",
    "                # bigram counts\n",
    "                for i in range(len(tokens) - 1):\n",
    "                    bigram_key = \"token1=%s_token2=%s\" % (tokens[i], tokens[i+1])\n",
    "                    self.feature_count[bigram_key] += 1\n",
    "                    curr_tweet_feature_count[bigram_key] += 1\n",
    "                    \n",
    "#                 # trigram counts\n",
    "#                 for i in range(len(tokens) - 2):\n",
    "#                     trigram_key = \"token1=%s_token2=%s_token3=%s\" % (tokens[i], tokens[i+1], tokens[i+2])\n",
    "#                     self.feature_count[trigram_key] += 1\n",
    "#                     curr_tweet_feature_count[trigram_key] += 1\n",
    "                    \n",
    "                data.append([conv_dict[fields[1]], curr_tweet_feature_count])\n",
    "\n",
    "        # generate features\n",
    "        print \"Number of training tweets\", self.total_count\n",
    "        print \"Number of features\", len(self.feature_count)\n",
    "        self.labels = []\n",
    "        self.training_data = []\n",
    "        for item in data:\n",
    "            self.labels.append(item[0])\n",
    "            curr_features = []\n",
    "            for token, count in self.feature_count.iteritems():\n",
    "                curr_features.append(item[1][token])\n",
    "            self.training_data.append(curr_features)\n",
    "        self.training_data = np.array(self.training_data)\n",
    "        self.labels = np.array(self.labels)\n",
    "    \n",
    "#     def train_pipeline(self, num_iter=10, eta=0.01):\n",
    "#         self.base_clf = linear_model.Perceptron(n_iter=num_iter, eta0=eta)\n",
    "#         self.classifier = Pipeline([\n",
    "#             ('vectorizer', CountVectorizer(min_n=1,max_n=2)),\n",
    "#             ('tfidf', TfidfTransformer()),\n",
    "#             ('clf', OneVsRestClassifier(self.base_clf))])\n",
    "#         self.classifier.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def train(self, num_iter=10, eta=0.01):\n",
    "        self.base_clf = linear_model.Perceptron(n_iter=num_iter, eta0=eta)\n",
    "        self.clf = OneVsRestClassifier(self.base_clf)\n",
    "        self.clf.fit(self.training_data, self.labels)\n",
    "        \n",
    "    def predict(self, tweet):\n",
    "        curr_tweet_feature_count = defaultdict(int)\n",
    "        curr_features = []\n",
    "        tokens = tweet.split(' ')\n",
    "        for token in tokens:\n",
    "            curr_tweet_feature_count[token] += 1\n",
    "            \n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigram_key = \"token1=%s_token2=%s\" % (tokens[i], tokens[i+1])\n",
    "            curr_tweet_feature_count[bigram_key] += 1\n",
    "            \n",
    "#         for i in range(len(tokens) - 2):\n",
    "#             trigram_key = \"token1=%s_token2=%s_token3=%s\" % (tokens[i], tokens[i+1], tokens[i+2])\n",
    "#             curr_tweet_feature_count[trigram_key] += 1\n",
    "            \n",
    "        for feature, count in self.feature_count.iteritems():\n",
    "            curr_features.append(curr_tweet_feature_count[feature])\n",
    "        return self.targets[self.clf.predict(np.array(curr_features).reshape(1, -1))]\n",
    "    \n",
    "#     def do_eval(self, test_file, start=None, end=None):\n",
    "#         with open(test_file, 'r') as f:\n",
    "#             lines = f.read().splitlines()\n",
    "#             start = start if start else 1\n",
    "#             end = end+1 if end else len(lines)\n",
    "#             for line in lines[start:end]:\n",
    "#                 fields = line.split('\\t')\n",
    "#                 self.X_test.append(fields[2])\n",
    "#                 self.y_test.append(fields[1])\n",
    "#         self.X_test = np.array(self.X_test)\n",
    "#         self.y_test = np.array(self.y_test)\n",
    "#         predicted = self.classifier.predict(self.X_test)\n",
    "#         return np.sum(predicted == self.y_test) / float(len(self.y_test))\n",
    "            \n",
    "        \n",
    "    def eval(self, test_file, start=None, end=None):\n",
    "        correct = 0\n",
    "        incorrect = 0\n",
    "        with open(test_file, 'r') as f:\n",
    "            lines = f.read().splitlines()\n",
    "            start = start if start else 1\n",
    "            end = end+1 if end else len(lines)\n",
    "            for line in lines[start:end]:\n",
    "                fields = line.split('\\t')\n",
    "                pred_correct = self.predict(fields[2]) == fields[1]\n",
    "                if pred_correct:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    incorrect += 1\n",
    "        return correct/float(incorrect+correct)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training tweets 1000\n",
      "Number of features 26302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tlaminator/anaconda/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:109: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.241793434748\n"
     ]
    }
   ],
   "source": [
    "# PATH_TO_DATA = os.path.join(os.getcwd(), 'twitter_dataset')\n",
    "PATH_TO_DATA = 'twitter_dataset'\n",
    "TRAIN_DIR, TRAIN_FILE = os.path.join(PATH_TO_DATA, 'train'), \"trainingdata-all-annotations.txt\"\n",
    "TEST_DIR, TEST_FILE = os.path.join(PATH_TO_DATA, 'test'), \"testdata-taskA-all-annotations.txt\"\n",
    "\n",
    "train_path = os.path.join(TRAIN_DIR, TRAIN_FILE)\n",
    "test_path = os.path.join(TEST_DIR, TEST_FILE)\n",
    "\n",
    "##### OWN IMPLEMENTATION #####\n",
    "# train_tweets, train_gold_labels, train_label_counts, train_stances, train_file_size = process_file(os.path.join(TRAIN_DIR, TRAIN_FILE), end=600)\n",
    "# test_tweets, test_gold_labels, test_label_counts, test_stances, test_file_size = process_file(os.path.join(TEST_DIR, TEST_FILE))\n",
    "# print \"Training label counts\", train_label_counts\n",
    "# ppn = Perceptron()\n",
    "# ppn.fit(train_tweets, train_gold_labels)\n",
    "\n",
    "##### SKLEARN IMPLEMENTATION #####\n",
    "skppn = PerceptronSKLearn()\n",
    "# skppn.vectorize_data(train_path)\n",
    "# skppn.train_pipeline(num_iter=50, eta=0.1)\n",
    "# acc = skppn.do_eval(test_path)\n",
    "# print acc\n",
    "\n",
    "\n",
    "skppn.prep_data(train_path, end=1000)\n",
    "skppn.train(num_iter=50, eta=0.1)\n",
    "acc = skppn.eval(test_path)\n",
    "print acc\n",
    "\n",
    "##### CHECK #####\n",
    "# print features_per_tweet(\"@NaughtyBeyotch @TheRealMadman23  Don't care for #Fiorina, but it seems she taking the #sexist gut punches by the #media. #MSM\", \"Hillary Clinton\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
